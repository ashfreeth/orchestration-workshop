<!DOCTYPE html>
<html>
  <head>
    <base target="_blank">
    <title>Docker Orchestration Workshop</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }

      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
        margin-top: 0.5em;
      }
      a {
        text-decoration: none;
        color: blue;
      }
      .remark-slide-content { padding: 1em 2.5em 1em 2.5em; }

      .remark-slide-content { font-size: 25px; }
      .remark-slide-content h1 { font-size: 50px; }
      .remark-slide-content h2 { font-size: 50px; }
      .remark-slide-content h3 { font-size: 25px; }
      .remark-code { font-size: 25px; }
      .small .remark-code { font-size: 16px; }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .red { color: #fa0000; }
      .gray { color: #ccc; }
      .small { font-size: 70%; }
      .big { font-size: 140%; }
      .underline { text-decoration: underline; }
      .pic {
        vertical-align: middle;
        text-align: center;
        padding: 0 0 0 0 !important;
      }
      img {
        max-width: 100%;
        max-height: 550px;
      }
      .title {
        vertical-align: middle;
        text-align: center;
      }
      .title h1 { font-size: 100px; }
      .title p { font-size: 80px; }
      .quote {
        background: #eee;
        border-left: 10px solid #ccc;
        margin: 1.5em 10px;
        padding: 0.5em 10px;
        quotes: "\201C""\201D""\2018""\2019";
        font-style: italic;
      }
      .quote:before {
        color: #ccc;
        content: open-quote;
        font-size: 4em;
        line-height: 0.1em;
        margin-right: 0.25em;
        vertical-align: -0.4em;
      }
      .quote p {
        display: inline;
      }
      .warning {
        background-image: url("warning.png");
        background-size: 1.5em;
        background-repeat: no-repeat;
        padding-left: 2em;
      }
      .exercise {
        background-color: #eee;
        background-image: url("keyboard.png");
        background-size: 1.4em;
        background-repeat: no-repeat;
        background-position: 0.2em 0.2em;
        border: 2px dotted black;
      }
      .exercise::before {
        content: "Exercise";
        margin-left: 1.8em;
      }
      .tips {
        background-color: #eee;
        background-size: 1.4em;
        background-repeat: no-repeat;
        background-position: 0.2em 0.2em;
        border: 2px dotted red;
      }
      .tips::before {
        content: "Tips";
        margin-left: 0.2em;
        color: red;
      }
      li p { line-height: 1.25em; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: title
![LinuxONE](linuxone-logo.png)
<br/>Docker <br/> Orchestration <br/> Workshop

## (ashfreeth)

---

## Intro.

- Credit goes to J√©r√¥me Petazzoni (jpetazzo) who created the DockerCoins app
  and wrote the original workshop material.

- This adapted version of the orchestration workshop covers deploying and
  scaling the DockerCoins app on LinuxONE (i.e. Linux on z)

- This is still a work in progress and only covers part of the content from the
  original workshop...watch this space!

- All the content is publicly available on GitHub<br/>
(https://github.com/ashfreeth/orchestration-workshop)

<!--
Remember to change:
- the link below
- the "tweet my speed" hashtag in DockerCoins HTML
-->

---


<!--
grep '^# ' index.html | grep -v '<br' | tr '#' '-'
-->

## Agenda

- <b>Chapter 1 - Getting Started</b>

  - Pre-requirements
  - Overview of the DockerCoins app
  - Running the app with Docker Compose

- <b>Chapter 2 - Scaling The App</b>

  - Scaling out the services
  - Identifying bottlenecks

- <b>Chapter 3 - Stabilising The Environment</b>

  - Dealing with stateful services
  - Deploying a local registry
  - Scripting image building and pushing

---

class: title

# .small[Chapter 1] </br> Getting Started

---

# Pre-requirements

- Linux on z (s390x) machine with:

  - Docker (https://www.ibm.com/developerworks/linux/linux390/docker.html)

  - Docker Compose
  (https://github.com/linux-on-ibm-z/docs/wiki/Building-Docker-Compose)

  - Some Docker knowledge (Only a little!)

.tips[

- If you want to manage the Docker service with systemd then add
  docker.service and docker.socket into /usr/lib/systemd/system/
  (Docker 1.11.2 -
  https://github.com/docker/docker/tree/v1.11.2/contrib/init/systemd)
- Don't forget to create the docker group and add your user to that group, so
  that you don't have to sudo to issue docker commands

]

---

## Nice-to-haves

- [GitHub](https://github.com/join) account
  <br/>(if you want to fork the repo)

- [Docker Hub](https://hub.docker.com) account
  <br/>(it's one way to distribute images to your cluster)

---

## Docker on z versions used

- Engine 1.11.2

- Compose 1.7.1

.exercise[

- Check all installed versions:
  ```bash
  docker version
  docker-compose -v
  ```

]
<br/>
.tips[

- The latest versions are available for SLES 12 and Ubuntu 16.04

]

---

# Our sample application

- Visit the GitHub repository with all the materials of this workshop:
  <br/>https://github.com/ashfreeth/orchestration-workshop

- The application is in the [dockercoins](
  https://github.com/ashfreeth/orchestration-workshop/tree/master/dockercoins)
  subdirectory

- Let's look at the general layout of the source code:

  there is a Compose file [docker-compose.yml](
  https://github.com/ashfreeth/orchestration-workshop/blob/master/dockercoins/docker-compose.yml) ...

  ... and 4 other services, each in its own directory:

  - <b>rng</b> = web service generating random bytes
  - <b>hasher</b> = web service computing hash of POSTed data
  - <b>worker</b> = background process using `rng` and `hasher`
  - <b>webui</b> = web interface to watch progress

---

## Compose file format version

*Particularly relevant if you have used Compose before...*

- Compose 1.6 introduced support for a new Compose file format (aka "v2")

- Services are no longer at the top level, but under a `services` section

- There has to be a `version` key at the top level, with value `"2"` (as a string, not an integer)

- Containers are placed on a dedicated network, making links unnecessary

- There are other minor differences, but upgrade is easy and straightforward

---

## Links, naming, and service discovery

- Containers can have network aliases (resolvable through DNS)

- Compose file version 2 makes each container reachable through its service name

- Compose file version 1 requires "links" sections

- Our code can connect to services using their short name

  (instead of e.g. IP address or FQDN)

---

## Example in `worker/worker.py`

![Service discovery](service-discovery.png)

---

.pic[
![DockerCoins logo](dockercoins.png)

(DockerCoins 2016 logo courtesy of [@XtlCnslt](https://twitter.com/xtlcnslt) and [@ndeloof](https://twitter.com/ndeloof). Thanks!)
]
---

## What's this application?

- It is a DockerCoin miner! üí∞üê≥üì¶üö¢

- No, you can't buy coffee with DockerCoins

- How DockerCoins works:

  - <b>worker</b> asks to <b>rng</b> to give it random bytes
  - <b>worker</b> feeds those random bytes into <b>hasher</b>
  - each hash starting with `0` is a DockerCoin
  - DockerCoins are stored in <b>redis</b>
  - <b>redis</b> is also updated every second to track speed
  - you can see the progress with the <b>webui</b>

---

## Getting the application source code

- We will clone the GitHub repository

- The repository also contains scripts and tools that we will use through the workshop

.exercise[

<!--
```bash
[ -d orchestration-workshop ] && mv orchestration-workshop orchestration-workshop.$$
```
-->

- Clone the repository on your Docker(s390x) machine :
  ```bash
  git clone git://github.com/ashfreeth/orchestration-workshop
  ```

]

(You can also fork the repository on GitHub and clone your fork if you prefer that.)

---

# Running the application

Without further ado, let's start our application.

.exercise[

- Go to the `dockercoins` directory, in the cloned repo:
  ```bash
  cd ~/orchestration-workshop/dockercoins
  ```

- Use Compose to build and run all containers:
  ```bash
  docker-compose up
  ```

]

Compose tells Docker to build all container images (pulling
the corresponding base images), then starts all containers,
and displays aggregated logs.

---

## Lots of logs

- The application continuously generates logs

- We can see the `worker` service making requests to `rng` and `hasher`

- Let's put that in the background

.exercise[

- Stop the application by hitting `^C`

<!--
```meta
^C
```
-->

]

- `^C` stops all containers by sending them the `TERM` signal

- Some containers exit immediately, others take longer
  <br/>(because they don't handle `SIGTERM` and end up being killed after a 10s timeout)

---

## Restarting in the background

- Many flags and commands of Compose are modeled after those of `docker`

.exercise[

- Start the app in the background with the `-d` option:
  ```bash
  docker-compose up -d
  ```

- Check that our app is running with the `ps` command:
  ```bash
  docker-compose ps
  ```

]

`docker-compose ps` also shows the ports exposed by the application.

---

## Viewing logs

- The `docker-compose logs` command works like `docker logs`

.exercise[

- View all logs since container creation and exit when done:
  ```bash
  docker-compose logs
  ```

- Stream container logs, starting at the last 10 lines for each container:
  ```bash
  docker-compose logs --tail 10 --follow
  ```

<!--
```meta
^C
```
-->

]

Tip: use `^S` and `^Q` to pause/resume log output.

???

## Upgrading from Compose 1.6

.warning[The `logs` command has changed between Compose 1.6 and 1.7!]

- Up to 1.6

  - `docker-compose logs` is the equivalent of `logs --follow`

  - `docker-compose logs` must be restarted if containers are added

- Since 1.7

  - `--follow` must be specified explicitly

  - new containers are automatically picked up by `docker-compose logs`

---

## Connecting to the web UI

- The `webui` container exposes a web dashboard; let's view it

.exercise[

- Open http://[yourVMaddr]:8000/ (from a browser)

]

- The app actually has a constant, steady speed (3.33 coins/second)

- The speed seems not-so-steady because:

  - the worker doesn't update the counter after every loop, but up to once per second

  - the speed is computed by the browser, checking the counter about once per second

  - between two consecutive updates, the counter will increase either by 4, or by 0

---

class: title

# .small[Chapter 2] </br> Scaling The App

---

## Scaling out the application

- Our goal is to make that performance graph go up (without changing a line of code!)

- Before trying to scale the application, we'll figure out if we need more resources

  (CPU, RAM...)

- For that, we will use good old UNIX tools on our Docker node

---

## Looking at resource usage

- Let's look at CPU, memory, and I/O usage

.exercise[

- run `top` to see CPU and memory usage (you should see idle cycles)

- run `vmstat 3` to see I/O usage (si/so/bi/bo)
  <br/>(the 4 numbers should be almost zero, except `bo` for logging)

]

We have available resources.

- Why?
- How can we use them?

---

## Scaling workers on a single node

- Docker Compose supports scaling
- Let's scale `worker` and see what happens!

.exercise[

- Start one more `worker` container:
  ```bash
  docker-compose scale worker=2
  ```

- Look at the performance graph (it should show a x2 improvement)

- Look at the aggregated logs of our containers (`worker_2` should show up)

- Look at the impact on CPU load with e.g. top (it should be negligible)

]

---

## Adding more workers

- Great, let's add more workers and call it a day, then!

.exercise[

- Start eight more `worker` containers:
  ```bash
  docker-compose scale worker=10
  ```

- Look at the performance graph: does it show a x10 improvement?

- Look at the aggregated logs of our containers

- Look at the impact on CPU load and memory usage

<!--
```bash
sleep 5
killall docker-compose
```
-->

]

---

# Identifying bottlenecks

- You should have seen a 3x speed bump (not 10x)

- Adding workers didn't result in linear improvement

- *Something else* is slowing us down

--

- ... But what?

--

- The code doesn't have instrumentation

- Let's use state-of-the-art HTTP performance analysis!
  <br/>(i.e. good old tools like `ab`, `httping`...)

---

## Finding the real cause of the bottleneck

- We want to debug our app as we scale `worker` up and down

- We want to run tools like `ab` or `httping` on the internal network

- .warning[This will be very hackish]

  (Better techniques and tools might become available in the future!)

---

## Creating a debug container

- Once our container is started (which should be really fast because the alpine image is small), we can enter it (from any node)

.exercise[

- Check the name of the network that the dockercoins app is using:
  ```bash
  docker network ls | grep dockercoins
  ```

- Create a container for debugging that is connected to the dockercoins network:
  ```bash
  docker run --net=dockercoins_default -ti brunswickheads/clefos72-base-s390x \
  /bin/bash
  ```

]

---

## Installing our debugging tools

- Ideally, you would author your own image, with all your favorite tools, and
use it instead of the base CentOS image from Docker Hub

- We are going to use some basic tools such as ping, nslookup and ApacheBench

.exercise[

- Install a few tools:
  ```bash
  yum install httpd-tools iputils bind-utils
  ```

]

---

## Investigating the `rng` service

- First, let's check what `rng` resolves to

.exercise[

- Use nslookup to resolve `rng`:
  ```bash
  nslookup rng
  ```

]

This give us one IP address. It is not the IP address of a container.
It is a virtual IP address (VIP) for the `rng` service.

---

## Investigating the VIP

.exercise[

- Try to ping the VIP:
  ```bash
  ping rng
  ```

]

It *should* ping. (But this might change in the future.)

Current behavior for VIPs is to ping when there is a backend available on the same machine.
(Again: this might change in the future.)

---

## Testing and benchmarking our service

- We will check that the service is up with `rng`, then
  benchmark it with `ab`

.exercise[

- Make a test request to the service:
  ```bash
  curl rng
  ```

- Open another window, and stop the workers, to test in isolation:
  ```bash
  docker-compose scale worker=0
  ```

]

Wait until the workers are stopped (check with `docker-compose ps`)
before continuing.

---

## Benchmarking `rng`

We will send 50 requests, but with various levels of concurrency.

.exercise[

- Send 50 requests, with a single sequential client:
  ```bash
  ab -c 1 -n 50 http://rng/10
  ```

- Send 50 requests, with fifty parallel clients:
  ```bash
  ab -c 50 -n 50 http://rng/10
  ```

]

---

## Benchmark results for `rng`

- When serving requests sequentially, they each take 100ms

- In the parallel scenario, the latency increased dramatically:

- What about `hasher`?

---

## Benchmarking `hasher`

We will do the same tests for `hasher`.

The command is slightly more complex, since we need to post random data.

First, we need to put the POST payload in a temporary file.

.exercise[

- Install curl in the container, and generate 10 bytes of random data:
  ```bash
  curl http://rng/10 >/tmp/random
  ```

]

---

## Benchmarking `hasher`

Once again, we will send 50 requests, with different levels of concurrency.

.exercise[

- Send 50 requests with a sequential client:
  ```bash
    ab -c 1 -n 50 -T application/octet-stream -p /tmp/random http://hasher/
  ```

- Send 50 requests with 50 parallel clients:
  ```bash
    ab -c 50 -n 50 -T application/octet-stream -p /tmp/random http://hasher/
  ```

]

---

## Benchmark results for `hasher`

- The sequential benchmarks takes ~5 seconds to complete

- The parallel benchmark takes less than 1 second to complete

- In both cases, each request takes a bit more than 100ms to complete

- Requests are a bit slower in the parallel benchmark

---

## Let's draw hasty conclusions

- The bottleneck seems to be `rng`

- It looks like `hasher` is better equiped to deal with concurrency than `rng`

- *What if* we don't have enough entropy and can't generate enough random numbers?

- Why does everythin take (at least) 100ms?

---

`rng` code:

![RNG code screenshot](delay-rng.png)

--

`hasher` code:

![HASHER code screenshot](delay-hasher.png)

---

class: title

But ...

WHY?!?

---

## Why did we sprinkle this sample app with sleeps?

- Deterministic performance
  <br/>(regardless of instance speed, CPUs, I/O...)

--

- Actual code sleeps all the time anyway

--

- When your code makes a remote API call:

  - it sends a request;

  - it sleeps until it gets the response;

  - it processes the response.

---

## Why do `rng` and `hasher` behave differently?

![Equations on a blackboard](equations.png)

--

(Synchronous vs. asynchronous event processing)

---

class: title

#  .small[Chapter 3] </br> Stabilising The Environment

---

## What's on the menu?

In this part, we will cover:

- Creating persistent volumes for the stateful services

- Building improved images for our app

- Shipping those images with a registry

---


# Dealing with stateful services

- First of all, you need to make sure that the data files are on a *volume*

- Volumes are host directories that are mounted to the container's filesystem

- These host directories can be backed by the ordinary, plain host filesystem ...

- ... Or by distributed/networked filesystems

- In the latter scenario, in case of node failure, the data is safe elsewhere ...

- ... And the container can be restarted on another node without data loss

---

## Building a stateful service experiment

- We will connect to our Dockercoins Redis database, which is running on port 6379

.exercise[

- Check that our Redis service for Dockercoins is running:
  ```bash
  docker ps | grep dockercoins_redis
  ```

-  Query the Redis server info using the redis-cli running within a container:
  ```bash
  docker run --rm --net="host" brunswickheads/redis-2.8.19-s390x redis-cli \
  -h 0.0.0.0 -p 6379 info server
  ```

]

---

## Accessing our Redis service easily

- Typing that whole command is going to be tedious

.exercise[

- Define a shell alias to make our lives easier:
  ```bash
  alias redis='docker run --rm --net="host" brunswickheads/redis-2.8.19-s390x \
  redis-cli -h 0.0.0.0 -p 6379'
  ```

- Try it:
  ```bash
  redis info server
  ```

]

---

## Basic Redis commands

.exercise[

- Check that the `foo` key doesn't exist:
  ```bash
  redis get foo
  ```

- Set it to `bar`:
  ```bash
  redis set foo bar
  ```

- Check that it exists now:
  ```bash
  redis get foo
  ```

]

---

## Local volumes vs. global volumes

- Global volumes exist in a single namespace

- A global volume can be mounted on any node
  <br/>.small[(bar some restrictions specific to the volume driver in use; e.g. using an EBS-backed volume on a GCE/EC2 mixed cluster)]

- Attaching a global volume to a container allows to start the container anywhere
  <br/>(and retain its data wherever you start it!)

- Global volumes require extra *plugins* (Flocker, Portworx...)

- Docker doesn't come with a default global volume driver at this point

- Therefore, we will fall back on *local volumes*

---

## Local volumes

- We will use the default volume driver, `local`

- As the name implies, the `local` volume driver manages *local* volumes

- Since local volumes are (duh!) *local*, in a cluster environment we would
need to pin our container to a specific host


---

## Where is our data?

- If we take down our Dockercoins app then all of the coin data will be lost!

.exercise[

- Query the Dockercoins wallet for coins:
  ```bash
  redis hkeys wallet
  ```
- Take down Dockercoins and bring it back up (~/orchestration-workshop/dockercoins/):
  ```bash
  docker-compose down; docker-compose up -d
  ```

- Check whether the same coins still exist:
  ```bash
  redis hkeys wallet
  ```

]

---

## Assigning a persistent volume to the container

- Let's add an explicit volume mount to our Redis service, so that the data can persist across container restarts

- Best practice would be to do this on a dedicated disk. This could be a FC (FCP)
attached volume or a virtual disk (e.g. minidisk, virtio)

.exercise[

- Attach a dedicated disk to your Docker host and identify the disk path, for example:
  ```bash
  lsblk -p
  ```

- Check the existing Docker volume list:
  ```bash
  docker volume ls
  ```

]

---

## Define the volume in our our Docker Compose service for Dockercoins

.exercise[

- Add a volume definition to the end of docker-compose.yml with your disk path
specified in 'device:' (~/orchestration-workshop/dockercoins/):

![Volume definition](volume-definition.png)

(An example of this is shown in docker-compose.yml-volumeExample)

]

---

## Configure Redis to mount the volume and specify options to ensure Redis stores data in that directory

.exercise[

- Add the 'volumes:' and 'command:' section to the redis service in docker-compose.yml
(~/orchestration-workshop/dockercoins/):

![Redis volume](redis-volume.png)

]

---

## Checking that persistence actually works across restarts

.exercise[

- Query the Dockercoins wallet for coins:
  ```bash
  redis hkeys wallet
  ```
- Take down Dockercoins and bring it back up (~/orchestration-workshop/dockercoins/):
  ```bash
  docker-compose down; docker-compose up -d
  ```

- Check whether the same coins still exist:
  ```bash
  redis hkeys wallet
  ```

]

---

## Recap

- The service must commit its state to disk when being shutdown

  (Shutdown = being sent a `TERM` signal)

- The state must be written on files located on a volume

- That volume must be specified to be persistent

- If using a local volume, the service must also be pinned to a specific node

  (And losing that node means losing the data, unless there are other backups)

---

class: title

#  .red[To Be Completed]

---

## Why do we need to ship our images?

- When we do `docker-compose up`, images are built for our services

- Those images are present only on the local node and would not be available
across a cluster

- The easiest way to achieve that is to use a Docker registry

- Once our images are on a registry, we can reference them when
  creating our services

---

## Build, ship, and run, for a single service

If we had only one service (built from a `Dockerfile` in the
current directory), our workflow could look like this:

```
docker build -t jpetazzo/doublerainbow:v0.1 .
docker push jpetazzo/doublerainbow:v0.1
docker service create jpetazzo/doublerainbow:v0.1
```

We just have to adapt this to our application, which has 4 services!

---

## The plan

- Build on our Docker node

- Tag images with a version number

  (timestamp; git hash; semantic...)

- Upload them to a registry

- Create services using the images

---

## Which registry do we want to use?

.small[

- **Docker Hub**

  - hosted by Docker Inc.
  - requires an account (free, no credit card needed)
  - images will be public (unless you pay)
  - located in AWS EC2 us-east-1

- **Docker Trusted Registry**

  - self-hosted commercial product
  - requires a subscription (free 30-day trial available)
  - images can be public or private
  - located wherever you want

- **Docker open source registry**

  - self-hosted barebones repository hosting
  - doesn't require anything
  - doesn't come with anything either
  - located wherever you want

]

---

## Using Docker Hub

- Set the `DOCKER_REGISTRY` environment variable to your Docker Hub user name
  <br/>(the `build-tag-push.py` script prefixes each image name with that variable)

- We will also see how to run the open source registry
  <br/>(so use whatever option you want!)

.exercise[

<!--
```meta
^{
```
-->

- Set the following environment variable:
  <br/>`export DOCKER_REGISTRY=jpetazzo`

- (Use *your* Docker Hub login, of course!)

- Log into the Docker Hub:
  <br/>`docker login`

<!--
```meta
^}
```
-->

]

---

## Using Docker Trusted Registry

If we wanted to use DTR, we would:

- make sure we have a Docker Hub account
- [activate a Docker Datacenter subscription](
  https://hub.docker.com/enterprise/trial/)
- install DTR on our machines
- set `DOCKER_REGISTRY` to `dtraddress:port/user`

*This is out of the scope of this workshop!*

---

## Using open source registry

- We need to run a registry container

- It will store images and layers to the local filesystem
  <br/>(but you can add a config file to use S3, Swift, etc.)

- Docker *requires* TLS when communicating with the registry

  - unless for registries on `localhost`

  - or with the Engine flag `--insecure-registry`

- Our strategy: publish the registry container on port 5000 and connect to it
through `localhost:5000` on each node

---

# Deploying a local registry

- We will create a single-instance service, publishing its port
  on the whole cluster

.exercise[

- Create the registry service:
  ```bash
  docker run -d --name registry --publish 5000:5000 \
  brunswickheads/registry-0.9.1-s390x
  ```

- Check the status of the repository `(it should respond with {})`:
  ```bash
  curl localhost:5000/v1/_ping
  ```

]

(Retry a few times, it might take 10-20 seconds for the container to be started. Patience.)

---

## Testing our local registry

- We can retag a small image, and push it to the registry

.exercise[

- Make sure we have the busybox image, and retag it:
  ```bash
  docker pull busybox
  docker tag busybox localhost:5000/busybox
  ```

- Push it:
  ```bash
  docker push localhost:5000/busybox
  ```

]

---

## Checking what's on our local registry

- The registry API has endpoints to query what's there

.exercise[

- Ensure that our busybox image is now in the local registry:
  ```bash
  curl http://localhost:5000/v2/_catalog
  ```

]

The curl command should now output:
```json
{"repositories":["busybox"]}
```

---

## Build, tag, and push our application container images

- Scriptery to the rescue!

.exercise[

- Set `DOCKER_REGISTRY` and `TAG` environment variables to use our local registry

- And run this little for loop:
  ```bash
    DOCKER_REGISTRY=localhost:5000
    TAG=v0.1
    for SERVICE in hasher rng webui worker; do
      docker-compose build $SERVICE
      docker tag dockercoins_$SERVICE $DOCKER_REGISTRY/dockercoins_$SERVICE:$TAG
      docker push $DOCKER_REGISTRY/dockercoins_$SERVICE
    done
  ```

]

---

---

# Rolling updates

- We want to release a new version of the worker

- We will edit the code ...

- ... build the new image ...

- ... push it to the registry ...

- ... update our service to use the new image

---

## But first...

- Restart the workers

.exercise[

- Just scale back to 10 replicas:
  ```bash
  docker service update worker --replicas 10
  ```

- Check that they're running:
  ```bash
  docker service ps worker
  ```

]

---

## Making changes

.exercise[

- Edit `~/orchestration-workshop/dockercoins/worker/worker.py`

- Locate the line that has a `sleep` instruction

- Reduce the `sleep` from `0.1` to `0.01`

- Save your changes and exit

]

---

## Building and pushing the new image

.exercise[

- Build the new image:
  ```bash
  IMAGE=localhost:5000/dockercoins_worker:v0.01
  docker build -t $IMAGE worker
  ```

- Push it to the registry:
  ```bash
  docker push $IMAGE
  ```

]

Note how the build and push were fast (because caching).

---

## Watching the deployment process

- We will need to open a new window for this

.exercise[

- Look at our service status:
  ```bash
  watch -n1 "docker service ps worker | grep -v Shutdown.*Shutdown"
  ```

]

- `docker service ps worker` gives us all tasks
  <br/>(including the one whose current or desired state is `Shutdown`)

- Then we filter out the tasks whose current **and** desired state is `Shutdown`

- Future versions might have fancy filters to make that less tinkerish

---

## Updating to our new image

- Keep the `watch ...` command running!

.exercise[

- In the other window, update the service to the new image:
  ```bash
  docker service update worker --image $IMAGE
  ```

]

By default, SwarmKit does a rolling upgrade, one instance at a time.

---

# Scripting image building and pushing

- Earlier, we used some rather crude shell loops to build and push images

- Compose (and clever environment variables) can help us to make that easier

- When using Compose file version 2, you can specify *both* `build` and `image`:

```yaml
version: "2"

services:

  webapp:
    build: src/
    image: jpetazzo/webapp:${TAG}
```

Note: Compose tolerates empty (or unset) environment variables, but in this example,
`TAG` *must* be set, because `jpetazzo/webapp:` is not a valid image name.

---

## Updating the Compose file to specify image tags

- Let's update the Compose file for DockerCoins to make it easier to push it to our registry

.exercise[

- Go back to the `dockercoins` directory:
  ```bash
  cd ~/orchestration-workshop/dockercoins
  ```

- Edit `docker-compose.yml`, and update each service to add an `image` directive as follows:
  ```yaml
    rng:
      build: rng
      `image: ${REGISTRY_SLASH}rng${COLON_TAG}`
  ```

]

You can also directly use the file `docker-compose.yml-images`.

---

## Use the new Compose file

- We need to set `REGISTRY_SLASH` and `COLON_TAG` variables

- Then we can use Compose to `build` and `push`

.exercise[

- Set environment variables:
  ```bash
  export REGISTRY_SLASH=localhost:5000/
  export COLON_TAG=:v0.01
  ```

- Build and push with Compose:
  ```bash
  docker-compose build
  docker-compose push
  ```

]

---

## Why the weird variable names?

- It would be more intuitive to have:
  ```bash
  REGISTRY=localhost:5000
  TAG=v0.01
  ```

- But then, when the variables are not set, the image names would be invalid

  (they would look like .red[`/rng:`])

- Putting the slash and the colon in the variables allows to use the Compose file
  even when the variables are not set

- The variable names (might) remind you that you have to put the trailing slash and heading colon

---

class: title

# Thanks! <br/> Questions?

## [ashfreeth](mailto:ashfreet@uk.ibm.com)

    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-0.13.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true
      });
    </script>
  </body>
</html>
